{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import keras\n",
    "from keras.layers import Flatten, Input, Dense\n",
    "from keras.layers import Dropout, Conv1D, Activation, MaxPooling1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load datasets\n",
    "def get_datasets(diseases, nr_inputs=260):\n",
    "    datasets = []\n",
    "    sample_dir = \"datasets/samples\"\n",
    "    for idx, disease in enumerate(diseases):\n",
    "        dataset_dir = os.path.join(sample_dir, str(idx))\n",
    "        datasets.append([])\n",
    "        for record in sorted(os.listdir(dataset_dir)):\n",
    "            record_path = os.path.join(dataset_dir, record)\n",
    "            with open(record_path) as dis:\n",
    "                dataset = np.loadtxt(dis)\n",
    "                if len(dataset) != nr_inputs:\n",
    "                    print(len(dataset), nr_inputs)\n",
    "                    continue\n",
    "                datasets[idx].append(dataset)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(datasets):\n",
    "    normalized=[]\n",
    "    for case in datasets:\n",
    "        cases=[]\n",
    "        for data in case:\n",
    "            mu = np.mean(data)\n",
    "            sigma = np.std(data)\n",
    "            #fixed = (np.array(data) - mu) / sigma\n",
    "            fixed = stats.zscore(data)\n",
    "            cases.append(fixed - min(fixed))\n",
    "        normalized.append(cases) \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data):\n",
    "    s_thd = np.random.uniform(low=0.15, high=0.25, size=None)\n",
    "    m_thd = np.random.uniform(low=-0.2, high=0.2, size=None)\n",
    "    mu, sigma = np.mean(data), np.std(data)\n",
    "    noise = np.random.normal(mu, sigma, [data.shape[0],]) * s_thd + m_thd\n",
    "    fixed = data + noise\n",
    "    return fixed-min(fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(datasets, data_length):\n",
    "    generate_data_num = [max(data_length) - x for x in data_length]\n",
    "    for idx, num in enumerate(generate_data_num):\n",
    "        for i in range(num):\n",
    "            target_idx = np.random.randint(data_length[idx], size=None)\n",
    "            target_data = datasets[idx][target_idx]\n",
    "            datasets[idx].append(generate_data(target_data).tolist())\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid(datasets, split=0.9):\n",
    "    data_length = [len(x) for x in datasets]\n",
    "    train_length = int(min(data_length) * split)\n",
    "    train_data = [x[:train_length] for x in datasets]\n",
    "    valid_data = [x[train_length:] for x in datasets]\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(data):\n",
    "    label= [[idx]*len(x) for idx, x in enumerate(data)]\n",
    "    label= label[0]+label[1]+label[2]+label[3]+label[4]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model(filter_num,filter_size, num_classes=5, \n",
    "                  activation='relu', maxpool_size=2, dim=260):\n",
    "    \n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Conv1D (kernel_size=filter_size,\n",
    "                          filters=filter_num, \n",
    "                          input_shape=(dim,1)))\n",
    "    cnn_model.add(Activation(activation))\n",
    "    cnn_model.add(MaxPooling1D(pool_size=(maxpool_size)))\n",
    "    \n",
    "    cnn_model.add(Conv1D(kernel_size=filter_size,\n",
    "                         filters=filter_num))\n",
    "    cnn_model.add(Activation(activation))\n",
    "    cnn_model.add(MaxPooling1D(pool_size=(maxpool_size)))\n",
    "    \n",
    "    cnn_model.add(Conv1D(kernel_size=filter_size,\n",
    "                         filters=filter_num))\n",
    "    cnn_model.add(Activation(activation))\n",
    "    cnn_model.add(MaxPooling1D(pool_size=(maxpool_size)))\n",
    "    \n",
    "    cnn_model.add(Conv1D(kernel_size=filter_size,\n",
    "                         filters=filter_num))\n",
    "    cnn_model.add(Activation(activation))\n",
    "    cnn_model.add(MaxPooling1D(pool_size=(maxpool_size)))\n",
    "    \n",
    "    cnn_model.add(Flatten())\n",
    "    cnn_model.add(Dense(12))\n",
    "    cnn_model.add(Activation(activation))\n",
    "    cnn_model.add(Dense(8))\n",
    "    cnn_model.add(Activation(activation))\n",
    "    cnn_model.add(Dense(num_classes))\n",
    "    cnn_model.add(Activation('softmax'))\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, x_train, y_train, x_test, y_test, opt, \n",
    "             batch_size=32, epochs=20, shuffle=True,\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'], verbose=0):\n",
    "    \n",
    "    model.compile(loss=loss, optimizer=opt, metrics=metrics)\n",
    "    history = model.fit(x_train, y_train, \n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        shuffle=shuffle,\n",
    "                       verbose=verbose)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict(model, datasets):\n",
    "    predict_list = []\n",
    "    for case in datasets:\n",
    "        predict_case=[]\n",
    "        for data in case:\n",
    "            data = np.expand_dims(data,1)\n",
    "            rs = np.argmax(model.predict(np.expand_dims(data,0)))\n",
    "            predict_case.append(rs)\n",
    "        predict_list.append(predict_case)\n",
    "    return predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_num(y_true, y_pred, label_num):\n",
    "    total_nums=[]\n",
    "    for a in range(label_num):\n",
    "        nums=[]\n",
    "        idx = np.array(y_true)==a\n",
    "        for case in range(label_num):\n",
    "            nums.append(sum(np.array(y_pred)[idx]==case))\n",
    "        total_nums.append(nums)\n",
    "    return total_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "heart_diseases = ['N', 'S', 'V', 'F', 'Q']\n",
    "num_classes=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_datasets(heart_diseases)\n",
    "datasets = normalize_data(datasets)\n",
    "#datasets = [x[:100] for x in datasets]\n",
    "data_length = [len(x) for x in datasets]\n",
    "datasets = generate_datasets(datasets, data_length)\n",
    "generated_data_length = [len(x) for x in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets\n",
    "train_label = get_label(train_data)\n",
    "train_data = [x for case in train_data for x in case]\n",
    "train_data = np.reshape(train_data, (len(train_data), 260, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_data, \n",
    "                                                    train_label,\n",
    "                                                    train_size=0.9)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = datasets\n",
    "valid_label = get_label(valid_data)\n",
    "valid_label = keras.utils.to_categorical(valid_label, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test case of filter size\n",
    "filter_size=[4,8,12]\n",
    "#test case of filter num\n",
    "filter_num=[8,16,32]\n",
    "#test epochs\n",
    "epochs=15\n",
    "#learning rate\n",
    "lr=0.01\n",
    "#optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=lr, decay=1e-6)\n",
    "\n",
    "for size in filter_size:\n",
    "    for num in filter_num:\n",
    "        print(\"======================================================\")\n",
    "\n",
    "        ##############################\n",
    "        #get cnn model\n",
    "        model = get_cnn_model(size, num)\n",
    "        print('[filter num]{}  [filter size]{}'.format(num,size))\n",
    "        print(\"------------------------------------------------------\")\n",
    "        \n",
    "        ##############################\n",
    "        #compile and fit model\n",
    "        print(\">>fitting model..\")\n",
    "        history = fit_model(model=model, \n",
    "                            x_train=x_train,\n",
    "                            y_train=y_train,\n",
    "                            x_test=x_test,\n",
    "                            y_test=y_test,\n",
    "                            opt=opt,\n",
    "                            epochs=epochs,\n",
    "                            verbose=0)\n",
    "        print(\">>fitting complete\")\n",
    "        \n",
    "        ##############################\n",
    "        #predict label\n",
    "        print(\">>predict model..\")\n",
    "        predict_list = get_predict(model, valid_data)\n",
    "        true_list = [[idx]*len(x) for idx,x in enumerate(predict_list)]\n",
    "        flatten_predict = [x for case in predict_list for x in case]\n",
    "        flatten_true = [x for case in  true_list for x in case]\n",
    "        r_nums = get_total_num(y_true=flatten_true,\n",
    "                              y_pred=flatten_predict,\n",
    "                              label_num=len(heart_diseases))\n",
    "        print(\">>predict complete\")\n",
    "        \n",
    "        ##############################\n",
    "        #data evaluation score\n",
    "        #label/list | precision | recall | f1-score | support\n",
    "        #       0 |       \n",
    "        #       1 |\n",
    "        #       2 |               score                 num\n",
    "        #       3 |\n",
    "        #       4 |  \n",
    "        print(\"[scores]\")\n",
    "        print(\"------------------------------------------------------\")\n",
    "        print(classification_report(flatten_true, flatten_predict,\n",
    "                                    target_names=heart_diseases))\n",
    "        \n",
    "        ##############################\n",
    "        #label/pred | 0 | 1 | 2 | 3 | 4 |\n",
    "        #  0 |                          => sum()=datasets num\n",
    "        #  1 |                          => sum()=datasets num\n",
    "        #  2 |         data num         => sum()=datasets num\n",
    "        #  3 |                          => sum()=datasets num\n",
    "        #  4 |                          => sum()=datasets num\n",
    "        ##############################\n",
    "        print('[origin/prediction]')\n",
    "        print(\"------------------------------------------------------\")\n",
    "        for r in r_nums:\n",
    "            print(r)\n",
    "            \n",
    "        print(\"======================================================\")\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
